Configuration file:

The header file generated from labview NI eclipse will be used to generate the
configuration file. This file will be used dynamically by the library. Then, the
header file will be discarded. This form was chosen to not compile the library.
We will have a single library for all CRIOs. However, each CRIO will have its own
configuration file.

BIs:

BIs are joined in an array of any size. The lib can read the array at once using
the NiFpga_ReadArrayBool function. Comparative results between a single 64-bit read
and an array of bool was done, and the results where considerably similar in terms
of time and area (read speed ~17 reads/ms). As an initial implementation, we opted
for the 64-bit implementation. If this implementation is deemed to be insufficient
in the future, a BI array can be implemented.

A timer is used to perform reads every X microseconds. Using this timer, we do not
overload the processor with a large amount of reads.

A mutex is used to lock a region composed of a fetching time, fetched value (cache).
This mutex is necessary so several threads do not perform unnecessary simultanous
accesses to the FPGA. The mutex blocks all other threads (i.e. thread safe).

This implementation is not process safe. That means that each process will have its
own session and its own mutex. The change that this will be happening is low, so no
emphasis has been given about this point. The current implementations seems to be
enough for our current needs. If a process safe implementation is deemed to be necessary,
there will be a need to implement a shared memory. An example of the shared memory can
be found in the eval folder. An interesting implementation was also found online when
googling for boost::interprocess or boost, shared memory.

Obtained results:
-----32 BIs in a 64-bit unsigned integer----
Slice total        948  / 10250            %
Register Slices    3373 / 82000            %
LUT slices         2686 / 41000            %
RAM                3    / 135              %


-------64 BIs in an array of size 64--------
Slice total        993  / 10250            %
Register Slices    3569 / 82000            %
LUT slices         2832 / 41000            %
RAM                3    / 135              %

BOs:

For each BO, it was chosen a single address. The reason is that the area was estimated
for a single BO and for 32 BOs, and the FPGA area consumption was MINOR.

Obtained results:
----------------Single BO-------------------
Slice total         911 / 10250         8.9%
Register Slices    2985 / 82000         3.6%
LUT slices         2448 / 41000         6.0%
RAM                 3   / 135           2.2%


----------------32 BOs----------------------
Slice total         964 / 10250         9.4%
Register Slices    3217 / 82000         3.9%
LUT slices         2611 / 41000         6.4%
RAM                 3   / 135           2.2%

As can be seen, the occupied slices by increasing the number of BOs is minor.

AO:
The analog outputs seem to have a stronger footprint in the FPGA. Nonetheless, AO
modules seem to come with a small number of outputs. In the AO implementation, the
AOs are connected to the Linux RT using a single precision variable; however, in
the FPGA, it has to be converted to fixed point. Apparently, NI uses DSPs for this
purpose. For now, this will be sufficient.

----------------4 AOs----------------------
Slice total        1543 / 10250        15.0%
Register Slices    5218 / 82000         6.4%
LUT slices         4751 / 41000        11.6%
RAM                 3   / 135           2.2%
DSP                 1   / 240           0.4%
